{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import uuid\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "batch_size = 200\n",
    "original_dim = 1000\n",
    "latent_dim = 100\n",
    "intermediate_dim = 500\n",
    "epochs = 100\n",
    "epsilon_std = 1.0\n",
    "\n",
    "def samp(z_mean,z_log_var):\n",
    "    epsilon = tf.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + tf.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "def total_loss(x, y,z_mean,z_log_var):\n",
    "    ent_loss =   tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=y),1)\n",
    "    \n",
    "    kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),1)\n",
    "    print(kl_loss.get_shape())\n",
    "    #recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "    # D_KL(Q(z|X) || P(z)); calculate in closed form as both dist. are Gaussian\n",
    "    #kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "    return [ent_loss,kl_loss]\n",
    "\n",
    "def split_into_sent (text):\n",
    "    strg = ''\n",
    "    for word in text:\n",
    "        strg += word\n",
    "        strg += ' '\n",
    "    strg_cleaned = strg.lower()\n",
    "    for x in ['\\n','\"',\"!\", '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','^',']','_','`','{','|','}','~','\\t']:\n",
    "        strg_cleaned = strg_cleaned.replace(x, '')\n",
    "    sentences = sent_tokenize(strg_cleaned)\n",
    "    return sentences\n",
    "\n",
    "def vectorize_sentences(sentences):\n",
    "    vectorized = []\n",
    "    for sentence in sentences:\n",
    "        byword = sentence.split()\n",
    "        concat_vector = []\n",
    "        for word in byword:\n",
    "            try:\n",
    "                concat_vector.append(w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        vectorized.append(concat_vector)\n",
    "    return vectorized\n",
    "\n",
    "def batch_generator(sources,batch_size=50):\n",
    "    idxs = np.random.permutation(np.arange(len(sources)))\n",
    "    n_batches = len(idxs) // batch_size\n",
    "    for batch_i in range(n_batches):\n",
    "        this_idxs = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "        this_sources = sources[this_idxs, :]\n",
    "        yield (this_sources)\n",
    "\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    data_concat = []\n",
    "    word_vecs = vectorize_sentences(sentence)\n",
    "    print(\"wordvec length:\",len(word_vecs))\n",
    "    for x in word_vecs:\n",
    "        data_concat.append(list(itertools.chain.from_iterable(x)))\n",
    "    zero_matr = np.zeros(mat_shape)\n",
    "    print(mat_shape)\n",
    "    print(len(data_concat))\n",
    "    zero_matr[0] = np.array(data_concat)\n",
    "    return zero_matr\n",
    "\n",
    "\n",
    "def print_sentence_with_w2v(sent_vect):\n",
    "    word_sent = ''\n",
    "    tocut = sent_vect\n",
    "    for i in range (int(len(sent_vect)/100)):\n",
    "        word_sent += w2v.most_similar(positive=[tocut[:100]], topn=1)[0][0]\n",
    "        word_sent += ' '\n",
    "        tocut = tocut[100:]\n",
    "    print(word_sent)\n",
    "    \n",
    "\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec\n",
    "\n",
    "\n",
    "def interpolate_b_points(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample\n",
    "\n",
    "\n",
    "def sent_2_sent(sess,sent1,sent2, batch, dim,z_mean,x_decoded):\n",
    "    a = sent_parse([sent1], (batch,dim))\n",
    "   \n",
    "    b = sent_parse([sent2], (batch,dim))\n",
    "    encode_a = sess.run(z_mean,feed_dict={x:a})\n",
    "    \n",
    "    encode_b = sess.run(z_mean,feed_dict={x:b})\n",
    "    intermediate_points = interpolate_b_points(encode_a[0], encode_b[0], 5)\n",
    "    \n",
    "    for point in intermediate_points:\n",
    "        \n",
    "        zero_matr = np.zeros((batch,latent_dim))\n",
    "        zero_matr[0] = np.array(point)\n",
    "        p = sess.run(x_decoded,feed_dict={z:zero_matr})[0]\n",
    "        print_sentence_with_w2v(p)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('apt_vectors.vec')\n",
    "data_concat = []\n",
    "\n",
    "\n",
    "with open('APT_sanitized.txt',\"r\",encoding='utf-8') as f:\n",
    "    text=f.readlines()\n",
    "vect = vectorize_sentences(text)\n",
    "data = [x for x in vect  if len(x) == 10]\n",
    "for x in data:\n",
    "    data_concat.append(list(itertools.chain.from_iterable(x)))\n",
    "    \n",
    "\n",
    "data_array = np.array(data_concat)\n",
    "np.random.shuffle(data_array)\n",
    "\n",
    "train = data_array[:60000]\n",
    "test = data_array[60000:80000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1000)\n",
      "(200, 1000)\n",
      "(200,)\n",
      "Epoch: 0\n",
      "Loss  1068.5624\n",
      "Epoch: 1\n",
      "Loss  1070.268\n",
      "Epoch: 2\n",
      "Loss  1066.4949\n",
      "Epoch: 3\n",
      "Loss  1064.9714\n",
      "Epoch: 4\n",
      "Loss  1067.8148\n",
      "Epoch: 5\n",
      "Loss  1058.9308\n",
      "Epoch: 6\n",
      "Loss  1063.102\n",
      "Epoch: 7\n",
      "Loss  1060.4624\n",
      "Epoch: 8\n",
      "Loss  1061.5067\n",
      "Epoch: 9\n",
      "Loss  1061.3883\n",
      "Epoch: 10\n",
      "Loss  1067.2474\n",
      "wordvec length: 1\n",
      "(200, 1000)\n",
      "1\n",
      "wordvec length: 1\n",
      "(200, 1000)\n",
      "1\n",
      "utilise utilise attack3 pilfered utilise attack3 darkhotel â€œuse 6aeb71d05a2f9b7c52ec06d65d838e82 Aurora8 \n",
      "utilise attack3 attack3 pilfered utilise pilfered darkhotel darkhotel 6aeb71d05a2f9b7c52ec06d65d838e82 Aurora8 \n",
      "utilise attack3 attack3 pilfered utilise pilfered darkhotel darkhotel 6aeb71d05a2f9b7c52ec06d65d838e82 darkhotel \n",
      "utilise attack3 attack3 pilfered utilise pilfered darkhotel darkhotel 6aeb71d05a2f9b7c52ec06d65d838e82 darkhotel \n",
      "6aeb71d05a2f9b7c52ec06d65d838e82 attack3 attack3 darkhotel utilise pilfered darkhotel darkhotel attack3 darkhotel \n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(shape=[batch_size, original_dim], dtype=tf.float32)\n",
    "h = tf.layers.dense(x,intermediate_dim , activation=tf.nn.relu)\n",
    "z_mean=tf.layers.dense(h,latent_dim,name=\"encoder\")\n",
    "z_log_var =tf.layers.dense(h,latent_dim)\n",
    "z=samp(z_mean,z_log_var)\n",
    "                             \n",
    "h_decoded = tf.layers.dense(z,intermediate_dim, activation=tf.nn.relu)\n",
    "x_decoded = tf.layers.dense(h_decoded,original_dim, activation=tf.nn.sigmoid,name=\"decoder\")\n",
    "print(x.get_shape())\n",
    "print(x_decoded.get_shape())\n",
    "\n",
    "\n",
    "xent_loss,kl_loss=total_loss(x,x_decoded,z_mean,z_log_var)\n",
    "cost = tf.reduce_mean(xent_loss + kl_loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.5).minimize(cost)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "epoch_i=0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            for it_i, (this_sources) in enumerate(batch_generator(train,batch_size)):\n",
    "                l,s,kl = sess.run([cost,xent_loss,kl_loss],feed_dict={x:this_sources})\n",
    "                \n",
    "                \n",
    "            if epoch_i==10:\n",
    "                coord.request_stop()\n",
    "            print(\"Epoch:\",epoch_i)                    \n",
    "            epoch_i=epoch_i+1\n",
    "            print(\"Loss \",l)\n",
    "            #print(\"sigmoid\",s)\n",
    "            #print(\"kl\",kl)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "            # One of the threads has issued an exception.  So let's tell all the\n",
    "            # threads to shutdown.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait until all threads have finished.\n",
    "    coord.join(threads)\n",
    "    sent_2_sent(sess,'already legitimately used in an Internet Explorer plugin open source','depending on Windows version . Returns failure or success with',batch=200,dim=1000,z_mean=z_mean,x_decoded=x_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
